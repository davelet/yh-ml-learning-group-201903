# 0322

## 激活函数的选择

激活函数好或坏，不能凭感觉定论。然而，根据问题的性质，我们可以为神经网络更快更方便地收敛作出更好的选择。

用于分类器时，Sigmoid函数及其组合通常效果更好。

由于梯度消失问题，有时要避免使用sigmoid和tanh函数。

ReLU函数是一个通用的激活函数，目前在大多数情况下使用。

如果神经网络中出现死神经元，那么PReLU函数就是最好的选择。

请记住，ReLU函数只能在隐藏层中使用。


一点经验：**你可以从ReLU函数开始，如果ReLU函数没有提供最优结果，再尝试其他激活函数**。

## tensorFlow 对象识别API的使用

安装完依赖后可以先使用models中的jupyter notebook文件尝试，可以加入自己找的图片放进去。
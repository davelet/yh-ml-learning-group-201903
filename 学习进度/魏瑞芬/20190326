半监督学习


Lable={(xi,yi)},Unlabled={(xi)}.并且数量上,L<<U.

（１）单独使用有标记样本,我们能够生成有监督分类算法
（2）单独使用无标记样本,我们能够生成无监督聚类算法
（3）两者都使用,我们希望在1中加入无标记样本,增强有监督分类的效果;同样的,我们希望在2中加入有标记样本,增强无监督聚类的效果.

假设条件：
（１）平滑假设（Smoothness Assumption）
位于稠密数据区域的两个距离很近的样例的类标签相似，也就是说，当两个样例被稠密数据区域中的边连接时，它们在很大的概率下有相同的类标签；相反地，当两个样例被稀疏数据区域分开时，它们的类标签趋于不同． 

（２）聚类假设（Cluster Assumption）
当两个样例位于同一聚类簇时，它们在很大的概率下有相同的类标签．这个假设的等价定义为低密度分离假设（Low Sensity Separation Assumption），即分类 决策边界应该穿过稀疏数据区域，而避免将稠密数 据区域的样例分到决策边界两侧．

（３）流形假设（Manifold Assumption）
将高维数据嵌入到低维流形中，当两个样例位于低维流形中的一个小局部邻域内时，它们具有相似的类标签。



方法：
方法1 self-training(自训练算法)
Repeat:
1.用L生成分类策略F;
2.用F分类U,计算误差
3.选取U的子集u,即误差小的,加入标记.L=L+u;
重复上述步骤,直到U为空集

方法2 generative models生成模型
假设所有数据都是由同一个潜在的模型生成的
给未标记数据打标签等价于学习潜在模型的参数，而未标记数据的标记则可看作模型的缺失参数
通常可基于EM算法进行极大似然估计求解

方法3 SVMs半监督支持向量机

方法4 graph-basedmethods图论方法
给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点，若两个样本之间的相似度很高（或相关性很强），则对应结点之间存在一条边，边的“强度”（strength）正比于样本之间的相似度（或相关性）。我们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚未染色。于是，半监督学就对应于“颜色”在图上扩散或传播的过程。由于一个图对应了一个矩阵，这使得我们能基于矩阵运算来进行半监督学习算法的推到和分析。
https://blog.csdn.net/wang_cheng_hei/article/details/13275875

方法5 multiview learing多视角算法
见20190322的文件




博客：
https://blog.csdn.net/yhdzw/article/details/22733371
